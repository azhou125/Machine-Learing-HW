{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import naive_bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  (a)\n",
    "Download Sentiment Labelled Sentences Data Set. There are three data ﬁles under the root folder. yelp_labelled.txt, amazon_cells_labelled.txt and imdb_labelled.txt. Parse each ﬁle with the speciﬁcations in readme.txt. Are the labels balanced? If not, what’s the ratio between the two labels? Explain how you process these ﬁles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       sentence\n",
      "score          \n",
      "0           500\n",
      "1           500\n",
      "       sentence\n",
      "score          \n",
      "0           500\n",
      "1           500\n",
      "       sentence\n",
      "score          \n",
      "0           500\n",
      "1           500\n",
      "       sentence\n",
      "score          \n",
      "0          1500\n",
      "1          1500\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "data1 = pd.read_csv('imdb_labelled.txt', delimiter=\"\\t\", names=['sentence', 'score'], quoting=csv.QUOTE_NONE, header = None, encoding='utf-8')\n",
    "data2 = pd.read_csv('amazon_cells_labelled.txt', names=['sentence', 'score'], quoting=csv.QUOTE_NONE, delimiter=\"\\t\", header = None, encoding='utf-8')\n",
    "data3 = pd.read_csv('yelp_labelled.txt', names=['sentence', 'score'], quoting=csv.QUOTE_NONE, delimiter=\"\\t\", header = None, encoding='utf-8')\n",
    "df = data1.append([data2, data3])\n",
    "print data1.groupby(['score']).count()\n",
    "print data2.groupby(['score']).count()\n",
    "print data3.groupby(['score']).count()\n",
    "print df.groupby(['score']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process each files by parsing them into dataframe by using tab as a delimiter. Then group each dataframe by 'score' and count them. We get the count of each label in each dataframe.\n",
    "The labels are balanced.\n",
    "Ratio between lable 0 and lable 1 is: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) \n",
    "Pick your preprocessing strategy. Since these sentences are online reviews, they may contain signiﬁcant amounts of noise and garbage. You may or may not want to do one or all of thefollowing. Explain the reasons for each of your decision (why or why not). <br>\n",
    "• Lowercase all of the words. <br>\n",
    "• Lemmatization of all the words(i.e.,convert every word to its root so that all of “running,” “run,” and “runs” are converted to “run” and and all of “good,” “well,” “better,” and “best” are converted to “good”; this is easily done using nltk.stem). <br>\n",
    "• Strip punctuation. <br>\n",
    "• Strip the stop words, e.g., “the”, “and”, “or”. <br>\n",
    "• Something else? Tell us about it. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Lowercase all the words: I will do that because uppercase and lowercase doesn't effect the meaning. (Except some abbreviations, which is rare.)<br>\n",
    "2) Lemmatization of all the words: I will do that because these words have almost same meaning.<br>\n",
    "3) Strip punctuation: I will do that since punctuation don't have much meaning/information.<br>\n",
    "4) Strip the stop words: I will do that since these stop words don't have much meaning/information.<br>\n",
    "5) What else: rule out the number words like \"one\" \"two\" \"three\", \"1\", \"2\", \"3\", which we think somewaht meaningless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(A):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    wnl = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    A = A.lower()                                       # 1) lower case\n",
    "    A = A.split()                                       # 2) split by space\n",
    "    A = \" \".join([w for w in A if not w in stop_words]) # 3) filter by stop_words to delete cases like \"shouldn't\"\n",
    "    A = nltk.word_tokenize(A)                           # 4) split by tokenize\n",
    "    ret = []                                            # 5) deal with case like 'apple.banana' which can't be split by tokenize\n",
    "    for elt in A:\n",
    "        ret+=elt.split('.')\n",
    "    ret = [w for w in ret if not w in stop_words]       # 6) filter by stop_words again\n",
    "    num_set_digit = ['0','1','2','3','4','5','6','7','8','9']\n",
    "    num_set_str = ['zero','one','two','three','four','five','six','seven','eight','nine']\n",
    "    \n",
    "                                                        # 7) delete the punctuation and number\n",
    "    ret = [''.join(c for c in s if (c not in string.punctuation and c not in num_set_digit)) for s in ret if (s not in num_set_str)] \n",
    "    ret = [w for w in ret if not w in stop_words]\n",
    "    ret = [s for s in ret if s]\n",
    "    for i,elt in enumerate(ret):\n",
    "        ret[i]=wnl.lemmatize(wnl.lemmatize(wnl.lemmatize(elt,'v'),'a'))\n",
    "        ret[i]=ps.stem(ret[i])\n",
    "    ret = [w for w in ret if not w in stop_words]\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1=data1.copy()\n",
    "df1['sentence'] = df1['sentence'].apply(preprocessing) \n",
    "df2=data2.copy()\n",
    "df2['sentence'] = df2['sentence'].apply(preprocessing) \n",
    "df3=data3.copy()\n",
    "df3['sentence'] = df3['sentence'].apply(preprocessing) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)\n",
    "Split training and testing set. In this assignment, for each ﬁle, please use the ﬁrst 400 instances for each label as the training set and the remaining 100 instances as testing set. In total, there are 2400 reviews for training and 600 reviews for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1 = df1[df1['score']==0][:400].append(df1[df1['score']==1][:400])\n",
    "test1 = df1[df1['score']==0][400:].append(df1[df1['score']==1][400:])\n",
    "train2 = df2[df2['score']==0][:400].append(df2[df2['score']==1][:400])\n",
    "test2 = df2[df2['score']==0][400:].append(df2[df2['score']==1][400:])\n",
    "train3 = df3[df3['score']==0][:400].append(df3[df3['score']==1][:400])\n",
    "test3 = df3[df3['score']==0][400:].append(df3[df3['score']==1][400:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train1.append([train2,train3])\n",
    "test = test1.append([test2,test3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = [0]*400+[1]*400+[0]*400+[1]*400+[0]*400+[1]*400\n",
    "test_label = [0]*100+[1]*100+[0]*100+[1]*100+[0]*100+[1]*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d)\n",
    "Bag of Words model. Extract features and then represent each review using bag of words model, i.e., every word in the review becomes its own element in a feature vector. In order to do this, ﬁrst, make one pass through all the reviews in the training set (Explain why we can’t use testing set at this point) and build a dictionary of unique words. Then,make another pass through the review in both the training set and testing set and count up the occurrences of each word in your dictionary. The ith element of a review’s feature vector is the number of occurrences of the ith dictionary word in the review. Implement the bag of words model and report feature vectors of any two reviews in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400L, 3379L)\n",
      "(600L, 3379L)\n"
     ]
    }
   ],
   "source": [
    "words_collection=train['sentence'].values\n",
    "words = set()\n",
    "for i in words_collection:\n",
    "    words = words.union(i)\n",
    "word_list = list(words)\n",
    "\n",
    "wordsList = []\n",
    "for i in range(train.shape[0]): #len(trainData)\n",
    "    words1 = [0] * len(word_list)\n",
    "    sub_list = words_collection[i]\n",
    "    for j in range(len(word_list)):\n",
    "        if word_list[j] in sub_list:\n",
    "            count = len([w for w in sub_list if w==word_list[j]])\n",
    "            words1[j] = count\n",
    "    wordsList.append(words1)\n",
    "wordsMatrix = np.matrix(wordsList)\n",
    "print wordsMatrix.shape\n",
    "\n",
    "df_train=pd.DataFrame(wordsMatrix, columns=word_list)\n",
    "\n",
    "words_collection=test['sentence'].values\n",
    "wordsList = []\n",
    "for i in range(test.shape[0]):     # len(trainData)\n",
    "    words = [0] * len(word_list)\n",
    "    sub_list = words_collection[i]\n",
    "    for j in range(len(word_list)):\n",
    "        if word_list[j] in sub_list:\n",
    "            words[j] = 1\n",
    "    wordsList.append(words)\n",
    "wordsMatrix = np.matrix(wordsList)\n",
    "print wordsMatrix.shape\n",
    "\n",
    "df_test=pd.DataFrame(wordsMatrix, columns=word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain why we can’t use testing set during the step of feature extraction: <br>\n",
    "Answer: \n",
    "Will result in biased performance estimates.<br>\n",
    "To get an unbiased performance estimate, the test data must not be used in any way to make choices about the model, including feature selection.<br>\n",
    "Also since including testing data into feature selection will generate more features. It will probably cause some level of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report feature vectors of any two reviews in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[[0,1]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e)\n",
    "Pick your post processing strategy. Since the vast majority of English words will not appear in most of the reviews, most of the feature vector elements will be 0. This suggests that we need a postprocessing or normalization strategy that combats the huge variance of the elements in the feature vector. You may want to use one of the following strategies. Whatever choices you make, explain why you made the decision.<br> \n",
    "• log-normalization. For each element of the feature vector x, transform it into f (x) = log(x+1).<br>\n",
    "• l1 normalization. Normalize the l1 norm of the feature vector, x'=x/|x|. <br>\n",
    "• l2 normalization. Normalize the l2 norm of the feature vector, x'=x/||x||. <br>\n",
    "• Standardize the data by subtracting the mean and dividing by the variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain: I pick the \"log-normalization\", because intuitively, \n",
    "it is true that the importance of a word increase when it appear twice in one review.\n",
    "However, appearing twice doesn't means importance of this word was doubled. \n",
    "According to our intuitive sense, the contribution to the significance of the word that it appears for the second time is lower than it appears for the first time. That is why I pick log-normalization since this transformation make more sense according to our intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_trans(x):\n",
    "    return np.log(x+1)\n",
    "df_train1 = df_train.copy()\n",
    "df_test1 = df_test.copy()\n",
    "df_train1 = df_train1.applymap(log_trans)\n",
    "df_test1 = df_test1.applymap(log_trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f)\n",
    "Sentiment prediction. Train a logistic regression model (you can use existing packages here) on the training set and test on the testing set. Report the classiﬁcation accuracy and confusion matrix. Inspecting the weight vector of the logistic regression, what are the words that play the most important roles in deciding the sentiment of the reviews? Repeat this with a Naive Bayes classiﬁer and compare performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the logistic regression model\n",
    "optimal_features = df_train1.columns\n",
    "X_optimal = df_train1[optimal_features]\n",
    "y = train_label\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_optimal, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82333333333333336"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the logistic model on the testing data set and get the prediction labels\n",
    "y_test_pred = logreg.predict(df_test1[optimal_features])\n",
    "def accuracy(Y_predict,Y_true):\n",
    "    return metrics.accuracy_score(Y_true, Y_predict)\n",
    "acc1 = accuracy(y_test_pred,test_label)\n",
    "acc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classiﬁcation accuracy: 0.82333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[260,  40],\n",
       "       [ 66, 234]], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_label, y_test_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix: <br>\n",
    "       [[260,  40]<br>\n",
    "        [ 66, 234]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'bad', u'poor', u'terribl', u'wast', u'slow', u'suck', u'aw', u'disappoint', u'pay', u'stupid', u'break', u'horribl', u'plot', u'start', u'piec', u'hour', u'bland', u'fail', u'avoid', u'rude']\n",
      "[-3.2812639168359525, -2.2952325396057014, -1.8830955400031202, -1.8383040390163916, -1.6722630273227566, -1.6590876495976508, -1.6563185505203799, -1.5306297468568248, -1.5274601219199613, -1.4907652807106695, -1.4704691175824878, -1.4693957172218488, -1.4105980791144417, -1.3545151205861194, -1.3505709792640193, -1.3380327701836003, -1.3366533717593363, -1.3332305293879845, -1.3077009646160229, -1.2901152960382396]\n",
      "[u'great', u'love', u'excel', u'delici', u'nice', u'amaz', u'fantast', u'best', u'awesom', u'beauti', u'happi', u'comfort', u'good', u'friendli', u'well', u'perfect', u'wonder', u'incred', u'price', u'cool']\n",
      "[3.5869427072843343, 3.1320609338342966, 2.5626991828511585, 2.3783504171011569, 2.2501784334299848, 2.2076555578318624, 2.0426724896413044, 1.9575433108119216, 1.9155577409091826, 1.8910153220435948, 1.761196493089376, 1.688207045798499, 1.5759415371444434, 1.5566423448593143, 1.5521054396629341, 1.5365707566970275, 1.4999175751306653, 1.4181468618024908, 1.3815519531354241, 1.3646135503027823]\n"
     ]
    }
   ],
   "source": [
    "words_list1=[]\n",
    "import_list1 =[]\n",
    "words_list2=[]\n",
    "import_list2 =[]\n",
    "\n",
    "sort_index = np.argsort(logreg.coef_[0])\n",
    "for i in range(20):\n",
    "    words_list1.append(df_train1.columns[sort_index[i]])\n",
    "    import_list1.append(logreg.coef_[0][sort_index[i]])\n",
    "    \n",
    "sort_index = np.argsort(logreg.coef_[0])[::-1]\n",
    "for i in range(20):\n",
    "    words_list2.append(df_train1.columns[sort_index[i]])\n",
    "    import_list2.append(logreg.coef_[0][sort_index[i]])\n",
    "print words_list1\n",
    "print import_list1\n",
    "print words_list2\n",
    "print import_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that play the most important roles in deciding the sentiment of the reviews:<br>\n",
    "[u'bad', u'poor', u'terribl', u'wast', u'slow', u'suck', u'aw', u'disappoint', u'pay', u'stupid', u'break', u'horribl', u'plot', u'start', u'piec', u'hour', u'bland', u'fail', u'avoid', u'rude']<br>\n",
    "Related importance:<br>\n",
    "[-3.2812639168359525, -2.2952325396057014, -1.8830955400031202, -1.8383040390163916, -1.6722630273227566, -1.6590876495976508, -1.6563185505203799, -1.5306297468568248, -1.5274601219199613, -1.4907652807106695, -1.4704691175824878, -1.4693957172218488, -1.4105980791144417, -1.3545151205861194, -1.3505709792640193, -1.3380327701836003, -1.3366533717593363, -1.3332305293879845, -1.3077009646160229, -1.2901152960382396]<br>\n",
    "Related importance:<br>\n",
    "[u'great', u'love', u'excel', u'delici', u'nice', u'amaz', u'fantast', u'best', u'awesom', u'beauti', u'happi', u'comfort', u'good', u'friendli', u'well', u'perfect', u'wonder', u'incred', u'price', u'cool']<br>\n",
    "[3.5869427072843343, 3.1320609338342966, 2.5626991828511585, 2.3783504171011569, 2.2501784334299848, 2.2076555578318624, 2.0426724896413044, 1.9575433108119216, 1.9155577409091826, 1.8910153220435948, 1.761196493089376, 1.688207045798499, 1.5759415371444434, 1.5566423448593143, 1.5521054396629341, 1.5365707566970275, 1.4999175751306653, 1.4181468618024908, 1.3815519531354241, 1.3646135503027823]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2400, 3379)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_optimal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82333333333333336"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes:\n",
    "clf = naive_bayes.BernoulliNB()\n",
    "clf.fit(X_optimal, y)\n",
    "y_test_pred2 = clf.predict(df_test1[optimal_features])\n",
    "acc2 = accuracy(y_test_pred2,test_label)\n",
    "acc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classiﬁcation accuracy: 0.82333"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[257,  43],\n",
       "       [ 63, 237]], dtype=int64)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_label, y_test_pred2, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix: <br>\n",
    "       [[257,  43]<br>\n",
    "       [ 63, 237]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'inexperi', u'disconnect', u'sync', u'unsatisfi', u'sand', u'heist', u'charismafre', u'ticker', u'ticket', u'verg', u'fat', u'reader', u'fals', u'godfath', u'neglig', u'grab', u'humili', u'sashimi', u'unbear', u'dollar']\n",
      "[-7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153]\n",
      "[u'great', u'good', u'film', u'phone', u'love', u'movi', u'work', u'well', u'like', u'best', u'food', u'make', u'realli', u'place', u'time', u'excel', u'nice', u'go', u'servic', u'price']\n",
      "[-2.0041457798627693, -2.0418861078456159, -2.6972929604227138, -2.7742540015588428, -2.8150759960790976, -2.8290622380538375, -2.8576356104978933, -3.0844089298626818, -3.121450201543031, -3.1404983965137254, -3.1599164823708272, -3.1599164823708272, -3.2205411041872618, -3.2415945133850945, -3.3075524811768919, -3.3075524811768919, -3.3540724968117845, -3.3540724968117845, -3.378170048390845, -3.4028626609812167]\n"
     ]
    }
   ],
   "source": [
    "words_list1=[]\n",
    "import_list1 =[]\n",
    "words_list2=[]\n",
    "import_list2 =[]\n",
    "\n",
    "sort_index = np.argsort(clf.coef_[0])\n",
    "for i in range(20):\n",
    "    words_list1.append(df_train1.columns[sort_index[i]])\n",
    "    import_list1.append(clf.coef_[0][sort_index[i]])\n",
    "    \n",
    "sort_index = np.argsort(clf.coef_[0])[::-1]\n",
    "for i in range(20):\n",
    "    words_list2.append(df_train1.columns[sort_index[i]])\n",
    "    import_list2.append(clf.coef_[0][sort_index[i]])\n",
    "print words_list1\n",
    "print import_list1\n",
    "print words_list2\n",
    "print import_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that play the most important roles in deciding the sentiment of the reviews:<br>\n",
    "[u'inexperi', u'disconnect', u'sync', u'unsatisfi', u'sand', u'heist', u'charismafre', u'ticker', u'ticket', u'verg', u'fat', u'reader', u'fals', u'godfath', u'neglig', u'grab', u'humili', u'sashimi', u'unbear', u'dollar']\n",
    "[-7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153]\n",
    "[u'great', u'good', u'film', u'phone', u'love', u'movi', u'work', u'well', u'like', u'best', u'food', u'make', u'realli', u'place', u'time', u'excel', u'nice', u'go', u'servic', u'price']\n",
    "[-2.0041457798627693, -2.0418861078456159, -2.6972929604227138, -2.7742540015588428, -2.8150759960790976, -2.8290622380538375, -2.8576356104978933, -3.0844089298626818, -3.121450201543031, -3.1404983965137254, -3.1599164823708272, -3.1599164823708272, -3.2205411041872618, -3.2415945133850945, -3.3075524811768919, -3.3075524811768919, -3.3540724968117845, -3.3540724968117845, -3.378170048390845, -3.4028626609812167]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (g)\n",
    "N-gram model. Similar to the bag of words model, but now you build up a dictionary of ngrams, which are contiguous sequences of words. For example, “Alice fell down the rabbit hole” would then map to the 2-grams sequence: [\"Alice fell\", \"fell down\", \"down the\", \"the rabbit\",\"rabbithole\"], and all ﬁve of those symbols would be member softhen-gram dictionary. Try n=2, repeat (d)-(g) and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "(2400L, 10633L)\n",
      "(600L, 10633L)\n",
      "end\n",
      "Wall time: 1min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print 'start'\n",
    "def two_gram(list_of_words):\n",
    "    ret = []\n",
    "    if(len(list_of_words)<2):\n",
    "        return list_of_words\n",
    "    for i in range(len(list_of_words)-1):\n",
    "        ret.append(list_of_words[i]+' '+list_of_words[i+1])\n",
    "    return ret\n",
    "\n",
    "words_collection=train['sentence'].values\n",
    "words_two = set()\n",
    "for i in words_collection:\n",
    "    words_two = words_two.union(two_gram(i))\n",
    "word_list_two = list(words_two)\n",
    "\n",
    "wordsList_two = []\n",
    "for i in range(train.shape[0]): #len(trainData)\n",
    "    words_two = [0] * len(word_list_two)\n",
    "    sub_list = two_gram(words_collection[i])\n",
    "    for j in range(len(word_list_two)):\n",
    "        if word_list_two[j] in sub_list:\n",
    "            count = len([w for w in sub_list if w==word_list_two[j]])\n",
    "            words_two[j] = count\n",
    "    wordsList_two.append(words_two)\n",
    "wordsMatrix_two = np.matrix(wordsList_two)\n",
    "print wordsMatrix_two.shape\n",
    "\n",
    "df_train_two=pd.DataFrame(wordsMatrix_two, columns=word_list_two)\n",
    "\n",
    "words_collection=test['sentence'].values\n",
    "wordsList_two = []\n",
    "for i in range(test.shape[0]):     # len(trainData)\n",
    "    words_two = [0] * len(word_list_two)\n",
    "    sub_list = two_gram(words_collection[i])\n",
    "    for j in range(len(word_list_two)):\n",
    "        if word_list_two[j] in sub_list:\n",
    "            count = len([w for w in sub_list if w==word_list_two[j]])\n",
    "            words_two[j] = count\n",
    "    wordsList_two.append(words_two)\n",
    "wordsMatrix_two = np.matrix(wordsList_two)\n",
    "print wordsMatrix_two.shape\n",
    "\n",
    "df_test_two=pd.DataFrame(wordsMatrix_two, columns=word_list_two)\n",
    "\n",
    "df_train_two1 = df_train_two.copy()\n",
    "df_train_two1 = df_train_two1.applymap(log_trans)\n",
    "df_test_two1 = df_test_two.copy()\n",
    "df_test_two1 = df_test_two1.applymap(log_trans)\n",
    "print 'end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimal_features = df_train_two1.columns\n",
    "X_optimal = df_train_two1[optimal_features]\n",
    "y = train_label\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_optimal, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63666666666666671"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = logreg.predict(df_test_two1[optimal_features])\n",
    "acc2 = accuracy(y_test_pred,test_label)\n",
    "acc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.636666667"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[269,  31],\n",
       "       [187, 113]], dtype=int64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_label, y_test_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix:<br>\n",
    "    [[269,  31]<br>\n",
    "    [187, 113]]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'wast time', u'disappoint', u'wast money', u'custom servic', u'ever go', u'poor qualiti', u'stay away', u'piec junk', u'horribl', u'rat', u'bad phone', u'bad film', u'bad ever', u'buy product', u'realli bad', u'wait wait', u'make mistak', u'even bad', u'act bad', u'anytim soon']\n",
      "[-1.6444815091250979, -1.2855503906807295, -1.1904644557780506, -0.86593733385660054, -0.83844893270422161, -0.81449177317578758, -0.80894479262792474, -0.77163482414965578, -0.75944116231123826, -0.75944116231123826, -0.75525665583100299, -0.75175495638943091, -0.7454796755159071, -0.74059635096571419, -0.7240540258640854, -0.71217195190918381, -0.70123416877166989, -0.68061171733171044, -0.66954482917475155, -0.66455038697387958]\n",
      "[u'work great', u'highli recommend', u'great phone', u'great product', u'food good', u'realli good', u'easi use', u'great food', u'great film', u'reason price', u'good price', u'food delici', u'great servic', u'love place', u'film great', u'work fine', u'pretti good', u'well make', u'love', u'good product']\n",
      "[2.0307227755669195, 1.7491887827540205, 1.2796754398367292, 1.1787131919639442, 1.0702006992867952, 1.0695341840187276, 1.0020469987169651, 0.9703220313588089, 0.92408224795863603, 0.90895296550718518, 0.89200123011964405, 0.8819378547993203, 0.8783656346385359, 0.86171341154701209, 0.83238795128683918, 0.82670170073928551, 0.80310424663431712, 0.78276069062816844, 0.77861898713901612, 0.77093319037129282]\n"
     ]
    }
   ],
   "source": [
    "words_list1=[]\n",
    "import_list1 =[]\n",
    "words_list2=[]\n",
    "import_list2 =[]\n",
    "\n",
    "sort_index = np.argsort(logreg.coef_[0])\n",
    "for i in range(20):\n",
    "    words_list1.append(word_list_two[sort_index[i]])\n",
    "    import_list1.append(logreg.coef_[0][sort_index[i]])\n",
    "#     print df_train_two1.columns[sort_index[i]], logreg.coef_[0][sort_index[i]]\n",
    "# print ''\n",
    "sort_index = np.argsort(logreg.coef_[0])[::-1]\n",
    "for i in range(20):\n",
    "#     print df_train_two1.columns[sort_index[i]], logreg.coef_[0][sort_index[i]]\n",
    "    words_list2.append(word_list_two[sort_index[i]])\n",
    "    import_list2.append(logreg.coef_[0][sort_index[i]])\n",
    "print words_list1\n",
    "print import_list1\n",
    "print words_list2\n",
    "print import_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that play the most important roles in deciding the sentiment of the reviews:<br>\n",
    "[u'wast time', u'disappoint', u'wast money', u'custom servic', u'ever go', u'poor qualiti', u'stay away', u'piec junk', u'horribl', u'rat', u'bad phone', u'bad film', u'bad ever', u'buy product', u'realli bad', u'wait wait', u'make mistak', u'even bad', u'act bad', u'anytim soon']\n",
    "[-1.6444815091250979, -1.2855503906807295, -1.1904644557780506, -0.86593733385660054, -0.83844893270422161, -0.81449177317578758, -0.80894479262792474, -0.77163482414965578, -0.75944116231123826, -0.75944116231123826, -0.75525665583100299, -0.75175495638943091, -0.7454796755159071, -0.74059635096571419, -0.7240540258640854, -0.71217195190918381, -0.70123416877166989, -0.68061171733171044, -0.66954482917475155, -0.66455038697387958]\n",
    "[u'work great', u'highli recommend', u'great phone', u'great product', u'food good', u'realli good', u'easi use', u'great food', u'great film', u'reason price', u'good price', u'food delici', u'great servic', u'love place', u'film great', u'work fine', u'pretti good', u'well make', u'love', u'good product']\n",
    "[2.0307227755669195, 1.7491887827540205, 1.2796754398367292, 1.1787131919639442, 1.0702006992867952, 1.0695341840187276, 1.0020469987169651, 0.9703220313588089, 0.92408224795863603, 0.90895296550718518, 0.89200123011964405, 0.8819378547993203, 0.8783656346385359, 0.86171341154701209, 0.83238795128683918, 0.82670170073928551, 0.80310424663431712, 0.78276069062816844, 0.77861898713901612, 0.77093319037129282]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.63666666666666671"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes:\n",
    "clf = naive_bayes.BernoulliNB()\n",
    "clf.fit(X_optimal, y)\n",
    "y_test_pred2 = clf.predict(df_test_two1[optimal_features])\n",
    "acc2 = accuracy(y_test_pred2,test_label)\n",
    "acc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.63666666666666671"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[270,  30],\n",
       "       [188, 112]], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(test_label, y_test_pred2, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix:<br>\n",
    "    [[270,  30]<br>\n",
    "    [188, 112]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'littl els', u'obviou bluegreenscreen', u'low qualiti', u'servic receiv', u'waitress littl', u'alway old', u'entir movi', u'muffin come', u'see ok', u'redeem would', u'belowpar script', u'uneasi bad', u'never treat', u'superfici movi', u'work balanc', u'ear hurt', u'headset time', u'side restaur', u'feel cheat', u'time serv']\n",
      "[-7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153]\n",
      "[u'work great', u'highli recommend', u'work well', u'sound qualiti', u'film great', u'great phone', u'great food', u'go back', u'great product', u'come back', u'batteri life', u'great film', u'realli good', u'great servic', u'food good', u'ca nt', u'easi use', u'realli like', u'work fine', u'year ago']\n",
      "[-3.9562478991660033, -4.3836919139929424, -4.7891570221011071, -4.7891570221011071, -5.0123005734153168, -5.0123005734153168, -5.0123005734153168, -5.0123005734153168, -5.0123005734153168, -5.1458319660398395, -5.1458319660398395, -5.1458319660398395, -5.1458319660398395, -5.1458319660398395, -5.1458319660398395, -5.1458319660398395, -5.2999826458670984, -5.2999826458670984, -5.2999826458670984, -5.2999826458670984]\n"
     ]
    }
   ],
   "source": [
    "words_list1=[]\n",
    "import_list1 =[]\n",
    "words_list2=[]\n",
    "import_list2 =[]\n",
    "\n",
    "sort_index = np.argsort(clf.coef_[0])\n",
    "for i in range(20):\n",
    "    words_list1.append(word_list_two[sort_index[i]])\n",
    "    import_list1.append(clf.coef_[0][sort_index[i]])\n",
    "    \n",
    "sort_index = np.argsort(clf.coef_[0])[::-1]\n",
    "for i in range(20):\n",
    "    words_list2.append(word_list_two[sort_index[i]])\n",
    "    import_list2.append(clf.coef_[0][sort_index[i]])\n",
    "print words_list1\n",
    "print import_list1\n",
    "print words_list2\n",
    "print import_list2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words that play the most important roles in deciding the sentiment of the reviews:<br>\n",
    "[u'littl els', u'obviou bluegreenscreen', u'low qualiti', u'servic receiv', u'waitress littl', u'alway old', u'entir movi', u'muffin come', u'see ok', u'redeem would', u'belowpar script', u'uneasi bad', u'never treat', u'superfici movi', u'work balanc', u'ear hurt', u'headset time', u'side restaur', u'feel cheat', u'time serv']\n",
    "[-7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153, -7.091742115095153]\n",
    "[u'work great', u'highli recommend', u'work well', u'sound qualiti', u'film great', u'great phone', u'great food', u'go back', u'great product', u'come back', u'batteri life', u'great film', u'realli good', u'great servic', u'food good', u'ca nt', u'easi use', u'realli like', u'work fine', u'year ago']\n",
    "[-3.9562478991660033, -4.3836919139929424, -4.7891570221011071, -4.7891570221011071, -5.0123005734153168, -5.0123005734153168, -5.0123005734153168, -5.0123005734153168, -5.0123005734153168, -5.1458319660398395, -5.1458319660398395, -5.1458319660398395, -5.1458319660398395, -5.1458319660398395, -5.1458319660398395, -5.1458319660398395, -5.2999826458670984, -5.2999826458670984, -5.2999826458670984, -5.2999826458670984]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (h)\n",
    "PCA for bag of words model. The features in the bag of words model have large redundancy. Implement PCA to reduce the dimension of features calculated in (e) to 10, 50 and 100 respectively. Using these lower-dimensional feature vectors and repeat (f), (g). Report corresponding clustering and classiﬁcation results. (Note: You should implement PCA yourself, but you can usenumpy.svdor some other SVD package. Feel free to double-check your PCA implementation against an existing one) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "(U,s,Vh) = np.linalg.svd(df_train1,full_matrices=False, compute_uv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400L, 2400L)\n",
      "(2400L,)\n",
      "(2400L, 3379L)\n"
     ]
    }
   ],
   "source": [
    "print U.shape\n",
    "print s.shape\n",
    "print Vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr_train1=np.dot(df_train1,np.transpose(Vh[:10,:]))\n",
    "Xr_train2=np.dot(df_train1,np.transpose(Vh[:50,:]))\n",
    "Xr_train3=np.dot(df_train1,np.transpose(Vh[:100,:]))\n",
    "Xr_test1=np.dot(df_test1,np.transpose(Vh[:10,:]))\n",
    "Xr_test2=np.dot(df_test1,np.transpose(Vh[:50,:]))\n",
    "Xr_test3=np.dot(df_test1,np.transpose(Vh[:100,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.618333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[251,  49],\n",
       "       [180, 120]], dtype=int64)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the logistic regression model\n",
    "X_optimal = Xr_train1\n",
    "y = train_label\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_optimal, y)\n",
    "# Apply the logistic model on the testing data set and get the prediction labels\n",
    "y_test_pred = logreg.predict(Xr_test1)\n",
    "def accuracy(Y_predict,Y_true):\n",
    "    return metrics.accuracy_score(Y_true, Y_predict)\n",
    "acc1 = accuracy(y_test_pred,test_label)\n",
    "print acc1\n",
    "metrics.confusion_matrix(test_label, y_test_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.618<br>\n",
    "Confusion Matrix:<br>\n",
    "[[251,  49]<br>\n",
    "       [180, 120]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.606666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[190, 110],\n",
       "       [126, 174]], dtype=int64)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes:\n",
    "clf = naive_bayes.BernoulliNB()\n",
    "clf.fit(X_optimal, y)\n",
    "y_test_pred2 = clf.predict(Xr_test1)\n",
    "acc2 = accuracy(y_test_pred2,test_label)\n",
    "print acc2\n",
    "metrics.confusion_matrix(test_label, y_test_pred2, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.6067<br>\n",
    "Confusion Matrix:<br>\n",
    "       [[190, 110]<br>\n",
    "       [126, 174]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.695\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[256,  44],\n",
       "       [139, 161]], dtype=int64)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the logistic regression model\n",
    "X_optimal = Xr_train2\n",
    "y = train_label\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_optimal, y)\n",
    "# Apply the logistic model on the testing data set and get the prediction labels\n",
    "y_test_pred = logreg.predict(Xr_test2)\n",
    "def accuracy(Y_predict,Y_true):\n",
    "    return metrics.accuracy_score(Y_true, Y_predict)\n",
    "acc1 = accuracy(y_test_pred,test_label)\n",
    "print acc1\n",
    "metrics.confusion_matrix(test_label, y_test_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.695<br>\n",
    "Confusion Matrix:<br>\n",
    "[[256,  44]<br>\n",
    "[139, 161]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.626666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[213,  87],\n",
       "       [137, 163]], dtype=int64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes:\n",
    "clf = naive_bayes.BernoulliNB()\n",
    "clf.fit(X_optimal, y)\n",
    "y_test_pred2 = clf.predict(Xr_test2)\n",
    "acc2 = accuracy(y_test_pred2,test_label)\n",
    "print acc2\n",
    "metrics.confusion_matrix(test_label, y_test_pred2, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.626666666667<br>\n",
    "Confusion Matrix:<br>\n",
    "[[213,  87]<br>\n",
    "[137, 163]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[246,  54],\n",
       "       [117, 183]], dtype=int64)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the logistic regression model\n",
    "X_optimal = Xr_train3\n",
    "y = train_label\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_optimal, y)\n",
    "# Apply the logistic model on the testing data set and get the prediction labels\n",
    "y_test_pred = logreg.predict(Xr_test3)\n",
    "def accuracy(Y_predict,Y_true):\n",
    "    return metrics.accuracy_score(Y_true, Y_predict)\n",
    "acc1 = accuracy(y_test_pred,test_label)\n",
    "print acc1\n",
    "metrics.confusion_matrix(test_label, y_test_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.715<br>\n",
    "Confusion Matrix:<br>\n",
    "[[246,  54]<br>\n",
    "[117, 183]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[210,  90],\n",
       "       [120, 180]], dtype=int64)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes:\n",
    "clf = naive_bayes.BernoulliNB()\n",
    "clf.fit(X_optimal, y)\n",
    "y_test_pred2 = clf.predict(Xr_test3)\n",
    "acc2 = accuracy(y_test_pred2,test_label)\n",
    "print acc2\n",
    "metrics.confusion_matrix(test_label, y_test_pred2, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.65<br>\n",
    "Confusion Matrix:<br>\n",
    "[[210,  90]<br>\n",
    "[120, 180]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400L, 2400L)\n",
      "(2400L,)\n",
      "(2400L, 10633L)\n"
     ]
    }
   ],
   "source": [
    "df_train_two1\n",
    "(U,s,Vh) = np.linalg.svd(df_train_two1,full_matrices=False, compute_uv=True)\n",
    "print U.shape\n",
    "print s.shape\n",
    "print Vh.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xr_train1=np.dot(df_train_two1,np.transpose(Vh[:10,:]))\n",
    "Xr_train2=np.dot(df_train_two1,np.transpose(Vh[:50,:]))\n",
    "Xr_train3=np.dot(df_train_two1,np.transpose(Vh[:100,:]))\n",
    "Xr_test1=np.dot(df_test_two1,np.transpose(Vh[:10,:]))\n",
    "Xr_test2=np.dot(df_test_two1,np.transpose(Vh[:50,:]))\n",
    "Xr_test3=np.dot(df_test_two1,np.transpose(Vh[:100,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600L,)\n",
      "0.515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[297,   3],\n",
       "       [288,  12]], dtype=int64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the logistic regression model\n",
    "X_optimal = Xr_train1\n",
    "y = train_label\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_optimal, y)\n",
    "# Apply the logistic model on the testing data set and get the prediction labels\n",
    "y_test_pred = logreg.predict(Xr_test1)\n",
    "print y_test_pred.shape\n",
    "def accuracy(Y_predict,Y_true):\n",
    "    return metrics.accuracy_score(Y_true, Y_predict)\n",
    "acc1 = accuracy(y_test_pred,test_label)\n",
    "print acc1\n",
    "metrics.confusion_matrix(test_label, y_test_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.515<br>\n",
    "Confusion Matrix:<br>\n",
    "[[297,   3]<br>\n",
    "[288,  12]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.573333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[234,  66],\n",
       "       [190, 110]], dtype=int64)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes:\n",
    "clf = naive_bayes.BernoulliNB()\n",
    "clf.fit(X_optimal, y)\n",
    "y_test_pred2 = clf.predict(Xr_test1)\n",
    "acc2 = accuracy(y_test_pred2,test_label)\n",
    "print acc2\n",
    "metrics.confusion_matrix(test_label, y_test_pred2, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.573333333333<br>\n",
    "Confusion Matrix:<br>\n",
    "[[234,  66]<br>\n",
    "[190, 110]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.525\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[294,   6],\n",
       "       [279,  21]], dtype=int64)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the logistic regression model\n",
    "X_optimal = Xr_train2\n",
    "y = train_label\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_optimal, y)\n",
    "# Apply the logistic model on the testing data set and get the prediction labels\n",
    "y_test_pred = logreg.predict(Xr_test2)\n",
    "def accuracy(Y_predict,Y_true):\n",
    "    return metrics.accuracy_score(Y_true, Y_predict)\n",
    "acc1 = accuracy(y_test_pred,test_label)\n",
    "print acc1\n",
    "metrics.confusion_matrix(test_label, y_test_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.525<br>\n",
    "Confusion Matrix:<br>\n",
    "[[294,   6]<br>\n",
    "[279,  21]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[233,  67],\n",
       "       [191, 109]], dtype=int64)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes:\n",
    "clf = naive_bayes.BernoulliNB()\n",
    "clf.fit(X_optimal, y)\n",
    "y_test_pred2 = clf.predict(Xr_test2)\n",
    "acc2 = accuracy(y_test_pred2,test_label)\n",
    "print acc2\n",
    "metrics.confusion_matrix(test_label, y_test_pred2, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.57<br>\n",
    "Confusion Matrix:<br>\n",
    "[[233,  67]<br>\n",
    "[191, 109]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 100 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.538333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[287,  13],\n",
       "       [264,  36]], dtype=int64)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the logistic regression model\n",
    "X_optimal = Xr_train3\n",
    "y = train_label\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_optimal, y)\n",
    "# Apply the logistic model on the testing data set and get the prediction labels\n",
    "y_test_pred = logreg.predict(Xr_test3)\n",
    "def accuracy(Y_predict,Y_true):\n",
    "    return metrics.accuracy_score(Y_true, Y_predict)\n",
    "acc1 = accuracy(y_test_pred,test_label)\n",
    "print acc1\n",
    "metrics.confusion_matrix(test_label, y_test_pred, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.538333333333<br>\n",
    "Confusion Matrix:<br>\n",
    "[[287,  13]<br>\n",
    "[264,  36]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.561666666667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[234,  66],\n",
       "       [197, 103]], dtype=int64)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Naive Bayes:\n",
    "clf = naive_bayes.BernoulliNB()\n",
    "clf.fit(X_optimal, y)\n",
    "y_test_pred2 = clf.predict(Xr_test3)\n",
    "acc2 = accuracy(y_test_pred2,test_label)\n",
    "print acc2\n",
    "metrics.confusion_matrix(test_label, y_test_pred2, labels=None, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.561666666667<br>\n",
    "Confusion Matrix:<br>\n",
    "[[234,  66]<br>\n",
    "[197, 103]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
