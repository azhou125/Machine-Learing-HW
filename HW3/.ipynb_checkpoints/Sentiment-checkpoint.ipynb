{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence\n",
       "score          \n",
       "0          1500\n",
       "1          1500"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## (a)\n",
    "import csv\n",
    "data1 = pd.read_csv('imdb_labelled.txt', delimiter=\"\\t\", names=['sentence', 'score'], quoting=csv.QUOTE_NONE, header = None, encoding='utf-8')\n",
    "data2 = pd.read_csv('amazon_cells_labelled.txt', names=['sentence', 'score'], quoting=csv.QUOTE_NONE, delimiter=\"\\t\", header = None, encoding='utf-8')\n",
    "data3 = pd.read_csv('yelp_labelled.txt', names=['sentence', 'score'], quoting=csv.QUOTE_NONE, delimiter=\"\\t\", header = None, encoding='utf-8')\n",
    "df = data1.append([data2, data3])\n",
    "df.groupby(['score']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are balanced.\n",
    "Ratio between lable 0 and lable 1 is: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (b)\n",
    "# 1) Lowercase all the words: I will do that because uppercase and lowercase doesn't effect the meaning. (Except some abbreviations, which is rare.)\n",
    "# 2) Lemmatization of all the words: I will do that because these words have almost same meaning.\n",
    "# 3) Strip punctuation: I will do that since punctuation don't have much meaning/information.\n",
    "# 4) Strip the stop words: I will do that since these stop words don't have much meaning/information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(A):\n",
    "    A = A.lower()\n",
    "    wnl = WordNetLemmatizer()\n",
    "    A = nltk.word_tokenize(A)\n",
    "    ret = []\n",
    "    for elt in A:\n",
    "        ret.append(elt)\n",
    "    ret = [''.join(c for c in s if c not in string.punctuation) for s in ret]\n",
    "    ret = [s for s in ret if s]\n",
    "    for i,elt in enumerate(ret):\n",
    "        ret[i]=wnl.lemmatize(elt)\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    return [w for w in ret if not w in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=data1.copy()\n",
    "df1['sentence'] = df1['sentence'].apply(preprocessing) \n",
    "df2=data2.copy()\n",
    "df2['sentence'] = df2['sentence'].apply(preprocessing) \n",
    "df3=data3.copy()\n",
    "df3['sentence'] = df3['sentence'].apply(preprocessing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c)\n",
    "train1 = df1[df1['score']==0][:400].append(df1[df1['score']==1][:400])\n",
    "test1 = df1[df1['score']==0][400:].append(df1[df1['score']==1][400:])\n",
    "train2 = df2[df2['score']==0][:400].append(df2[df2['score']==1][:400])\n",
    "test2 = df2[df2['score']==0][400:].append(df2[df2['score']==1][400:])\n",
    "train3 = df3[df3['score']==0][:400].append(df3[df3['score']==1][:400])\n",
    "test3 = df3[df3['score']==0][400:].append(df3[df3['score']==1][400:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train1.append([train2,train3])\n",
    "test = test1.append([test2,test3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "(2400L, 4175L)\n",
      "end\n",
      "Wall time: 7.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# (d)\n",
    "print 'start'\n",
    "\n",
    "words_collection=train['sentence'].values\n",
    "# print ingredients_collection\n",
    "\n",
    "words = set()\n",
    "for i in words_collection:\n",
    "    words = words.union(i)\n",
    "    \n",
    "word_list = list(words)\n",
    "\n",
    "\n",
    "wordsList = []\n",
    "\n",
    "for i in range(train.shape[0]): #len(trainData)\n",
    "    words = [0] * len(word_list)\n",
    "    sub_list = words_collection[i]\n",
    "    for j in range(len(word_list)):\n",
    "        if word_list[j] in sub_list:\n",
    "            words[j] = 1\n",
    "    wordsList.append(words)\n",
    "\n",
    "wordsMatrix = np.matrix(wordsList)\n",
    "print wordsMatrix.shape\n",
    "print 'end'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ [u'slowmoving', u'aimless', u'movie', u'distressed', u'drifting', u'young', u'man']\n",
      " [u'sure', u'wa', u'lost', u'flat', u'character', u'audience', u'nearly', u'half', u'walked']\n",
      " [u'attempting', u'artiness', u'black', u'white', u'clever', u'camera', u'angle', u'movie', u'disappointed', u'became', u'even', u'ridiculous', u'acting', u'wa', u'poor', u'plot', u'line', u'almost', u'nonexistent']\n",
      " ..., [u'said', u'mouth', u'belly', u'still', u'quite', u'pleased']\n",
      " [u'2', u'thumb']\n",
      " [u'loved', u'grilled', u'pizza', u'reminded', u'legit', u'italian', u'pizza']]\n",
      "[[0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " ..., \n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]\n",
      " [0 0 0 ..., 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print words_collection\n",
    "print wordsMatrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
